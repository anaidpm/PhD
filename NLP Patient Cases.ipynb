{"metadata": {"language_info": {"codemirror_mode": {"version": 3, "name": "ipython"}, "nbconvert_exporter": "python", "file_extension": ".py", "version": "3.5.4", "mimetype": "text/x-python", "pygments_lexer": "ipython3", "name": "python"}, "kernelspec": {"display_name": "Python 3.5 with Spark", "language": "python3", "name": "python3"}}, "nbformat": 4, "cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "<h1>Text Classification Demo 1</h1>\n\n[Link here] (https://towardsdatascience.com/machine-learning-nlp-text-classification-using-scikit-learn-python-and-nltk-c52b92a7c73a)"}, {"metadata": {}, "cell_type": "markdown", "source": "<h2>Loading the training set</h2>"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": 4, "source": "from sklearn.datasets import fetch_20newsgroups\ntwenty_train = fetch_20newsgroups(subset='train', shuffle=True)"}, {"metadata": {}, "cell_type": "markdown", "source": "### View categories and one sample"}, {"outputs": [{"output_type": "stream", "text": "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc'] \n\nFrom: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\n", "name": "stdout"}], "metadata": {}, "cell_type": "code", "execution_count": 5, "source": "print(twenty_train.target_names,\"\\n\") #prints all the categories\nprint(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3])) #prints first line of the first data file"}, {"metadata": {}, "cell_type": "markdown", "source": "## Extracting features from text files"}, {"metadata": {}, "cell_type": "markdown", "source": "Scikit-learn has a high level component which will create feature vectors for us \u2018CountVectorizer\u2019."}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": 25, "source": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(twenty_train.data)\n#print(X_train_counts.shape)"}, {"metadata": {}, "cell_type": "markdown", "source": "Here by doing `count_vect.fit_transform(twenty_train.data)`, we are learning the vocabulary dictionary and it returns a Document-Term matrix. [n_samples, n_features]."}, {"metadata": {}, "cell_type": "markdown", "source": "TF: Just counting the number of words in each document has 1 issue: it will give more weightage to longer documents than shorter documents. To avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) / #Total words, in each document.\n\nTF-IDF: Finally, we can even reduce the weightage of more common words like (the, is, an etc.) which occurs in all document. This is called as TF-IDF i.e Term Frequency times inverse document frequency. below\n\nWe can achieve both using the code:"}, {"outputs": [{"output_type": "execute_result", "metadata": {}, "execution_count": 9, "data": {"text/plain": "(11314, 130107)"}}], "metadata": {}, "cell_type": "code", "execution_count": 9, "source": "from sklearn.feature_extraction.text import TfidfTransformer\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nX_train_tfidf.shape"}, {"metadata": {}, "cell_type": "markdown", "source": "## Creating pipeline"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": 19, "source": "from sklearn.pipeline import Pipeline\ntext_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),('clf', MultinomialNB()),])\ntext_clf = text_clf.fit(twenty_train.data, twenty_train.target)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Running ML algorithms"}, {"metadata": {}, "cell_type": "markdown", "source": "### Naive Bayes"}, {"metadata": {}, "cell_type": "markdown", "source": "**Train NB**"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": 14, "source": "from sklearn.naive_bayes import MultinomialNB\nnb_clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"}, {"metadata": {}, "cell_type": "markdown", "source": "__Evaluate NB__"}, {"outputs": [{"output_type": "execute_result", "metadata": {}, "execution_count": 20, "data": {"text/plain": "0.7738980350504514"}}], "metadata": {}, "cell_type": "code", "execution_count": 20, "source": "import numpy as np\ntwenty_test = fetch_20newsgroups(subset='test', shuffle=True)\npredicted = text_clf.predict(twenty_test.data)\nnp.mean(predicted == twenty_test.target)"}, {"metadata": {}, "cell_type": "markdown", "source": "### Support Vector Machines (SVM)"}, {"outputs": [{"output_type": "stream", "text": "/opt/ibm/conda/miniconda3/lib/python3.5/site-packages/sklearn/linear_model/stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n  DeprecationWarning)\n", "name": "stderr"}, {"output_type": "execute_result", "metadata": {}, "execution_count": 21, "data": {"text/plain": "0.82381837493361654"}}], "metadata": {}, "cell_type": "code", "execution_count": 21, "source": "from sklearn.linear_model import SGDClassifier\ntext_clf_svm = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                      ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n                                            alpha=1e-3, n_iter=5, random_state=42)),\n ])\n_ = text_clf_svm.fit(twenty_train.data, twenty_train.target)\npredicted_svm = text_clf_svm.predict(twenty_test.data)\nnp.mean(predicted_svm == twenty_test.target)"}, {"metadata": {}, "cell_type": "markdown", "source": "## Logistic Regression"}, {"outputs": [{"output_type": "execute_result", "metadata": {}, "execution_count": 27, "data": {"text/plain": "0.82793414763674988"}}], "metadata": {}, "cell_type": "code", "execution_count": 27, "source": "from sklearn.linear_model import LogisticRegression\ntext_clf_lr = Pipeline([('vect', CountVectorizer()),\n                      ('tfidf', TfidfTransformer()),\n                      ('clf-lr', LogisticRegression()),\n ])\n_ = text_clf_lr.fit(twenty_train.data, twenty_train.target)\npredicted_lr = text_clf_lr.predict(twenty_test.data)\nnp.mean(predicted_lr == twenty_test.target)"}, {"metadata": {}, "cell_type": "markdown", "source": "### With Grid search"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": " from sklearn.model_selection import GridSearchCV\nparameters_lr = {'vect__ngram_range': [(1, 1), (1, 2)],\n             'tfidf__use_idf': (True, False),\n               'clf-lr__penalty': ['l1','l2'],\n}\ngs_clf_lr = GridSearchCV(text_clf_lr, parameters_lr, n_jobs=-1)\ngs_clf_lr = gs_clf_lr.fit(twenty_train.data, twenty_train.target)\ngs_clf_lr.best_score_\ngs_clf_lr.best_params_"}, {"outputs": [], "metadata": {}, "cell_type": "code", "execution_count": null, "source": ""}], "nbformat_minor": 1}